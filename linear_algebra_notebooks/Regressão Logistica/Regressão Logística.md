
# Regress√£o Log√≠stica

**Recursos de Aprendizado:** Mathematics for Machine Learning (Peter Deisenroth), CS229, MIT 18.650 Statistics for Applications, Fall 2016

---

## Introdu√ß√£o

A regress√£o log√≠stica faz parte de uma classe de algoritmos de **classifica√ß√£o supervisionada**, sendo considerado o modelo mais simples de classifica√ß√£o bin√°ria.

---

## Bases Matem√°ticas

Dado um conjunto de atributos $X$ e uma resposta bin√°ria $Y$, temos que $Y_i | X_i$ segue uma distribui√ß√£o de Bernoulli:

$$Y_i | X_i \sim \text{Bernoulli}(p_i)$$

onde $p_i = \sigma(\theta^T X_i)$ e $\sigma$ √© a fun√ß√£o sigmoide (log√≠stica):

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

### Por que usar fam√≠lias exponenciais?

A distribui√ß√£o de Bernoulli pertence √†s **fam√≠lias exponenciais**, o que garante que a fun√ß√£o de custo (log-likelihood negativa) seja **convexa**. Isso permite otimiza√ß√£o eficiente por m√©todos como:

- Gradient Descent (Batch ou Estoc√°stico)
- Newton-Raphson
- BFGS

### Premissa de Independ√™ncia

O modelo assume que as vari√°veis s√£o **estatisticamente independentes**:

$$E(XY) = E(X)E(Y)$$

> ‚ö†Ô∏è **Nota:** N√£o verifiquei essa premissa no meu modelo.

### Propriedades das Fam√≠lias Exponenciais

$$E\left(\frac{\partial \log p_\theta(y|x)}{\partial \theta}\right) = 0$$

$$E\left(\frac{\partial^2 \log p_\theta(y|x)}{\partial \theta^2}\right) = -E\left(\frac{\partial \log p_\theta(y|x)}{\partial \theta}\right)^2$$

> üí° **Dica:** A segunda propriedade √© conhecida como **Informa√ß√£o de Fisher** e √© fundamental para entender a vari√¢ncia dos estimadores.

---

## Dataset Utilizado

Dataset de doen√ßas card√≠acas dispon√≠vel no [Kaggle](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression):

```python
import kagglehub
dataset_path = kagglehub.dataset_download("dileep070/heart-disease-prediction-using-logistic-regression")
```

### Vari√°veis do Dataset

| Vari√°vel | Descri√ß√£o | Range |
|----------|-----------|-------|
| `male` | Sexo masculino | 0 - 1 |
| `age` | Idade | 32 - 70 anos |
| `education` | Escolaridade | 1.0 - 4.0 |
| `currentSmoker` | Fumante atual | 0 - 1 |
| `cigsPerDay` | Cigarros por dia | 0.0 - 70.0 |
| `BPMeds` | Medica√ß√£o para press√£o | 0.0 - 1.0 |
| `prevalentStroke` | Hist√≥rico de AVC | 0 - 1 |
| `prevalentHyp` | Hipertens√£o | 0 - 1 |
| `diabetes` | Diabetes | 0 - 1 |
| `totChol` | Colesterol total | 113.0 - 600.0 |
| `sysBP` | Press√£o sist√≥lica | 83.5 - 295.0 |
| `diaBP` | Press√£o diast√≥lica | 48.0 - 142.5 |
| `BMI` | √çndice de massa corporal | 15.54 - 56.8 kg/m¬≤ |
| `heartRate` | Frequ√™ncia card√≠aca | 44.0 - 143.0 BPM |
| `glucose` | Glicose | 40.0 - 394.0 mg/dL |

**Objetivo:** Predizer o risco de doen√ßa card√≠aca em 10 anos.

---

## An√°lise Explorat√≥ria de Dados

Constru√≠ uma classe auxiliar para an√°lise explorat√≥ria com Seaborn.

![An√°lise Explorat√≥ria](linear_algebra_notebooks/Regress√£o%20Logistica/doubleplot.png)

### Tratamento de Dados

- **Dados faltantes:** substitu√≠dos pela m√©dia das demais observa√ß√µes.

> üìö **Fonte:** Introdu√ß√£o √† Estat√≠stica, Mario F. Triola

---

## Lidando com Dados Desbalanceados

### Class Weights

Para mitigar o enviesamento, apliquei pesos inversamente proporcionais √† frequ√™ncia de cada classe no gradiente:

```python
def grad(self, theta):
	predictions = self.inv_Logistic_link(self.X @ theta)
	
	n_samples = len(self.Y)
	n_classes = 2
	n_class_0 = np.sum(self.Y == 0)
	n_class_1 = np.sum(self.Y == 1)
	
	# Peso inversamente proporcional √† frequ√™ncia
	weight_0 = n_samples / (n_classes * n_class_0)
	weight_1 = n_samples / (n_classes * n_class_1)
	
	weights = np.where(self.Y == 1, weight_1, weight_0)
	errors = (predictions - self.Y) * weights
	
	return self.X.T @ errors
```

> üí° **Conceito:** Ao dar mais peso para a classe minorit√°ria, for√ßamos o modelo a "prestar mais aten√ß√£o" nela durante o treinamento.

---

## Redu√ß√£o de Dimensionalidade com PCA

Utilizei PCA para identificar os componentes de maior vari√¢ncia. O melhor resultado foi obtido com **9 componentes**.

> ‚ö†Ô∏è Os dados s√£o muito desbalanceados e a distribui√ß√£o n√£o permite separar grupos claros, indicando limita√ß√µes no treinamento.

![PCA](linear_algebra_notebooks/Regress√£o%20Logistica/PCA.png)

---

## Visualiza√ß√£o da Superf√≠cie de Custo

A fun√ß√£o de perda √© convexa (c√¥ncava no caso da log-likelihood). Visualiza√ß√£o nos dois primeiros betas:

![Superf√≠cie de Custo](linear_algebra_notebooks/Regress√£o%20Logistica/output.png)

---

## Resultados

### Matriz de Confus√£o

|  | Predito Negativo | Predito Positivo |
|--|------------------|------------------|
| **Real Negativo** | 500 (VN) | 219 (FP) |
| **Real Positivo** | 52 (FN) | 77 (VP) |

### M√©tricas de Avalia√ß√£o

| M√©trica | Valor |
|---------|-------|
| Acur√°cia | 0.68 |
| Precision | 0.26 |
| Recall | 0.60 |
| F1 Score | 0.36 |

> üìñ **Entendendo as m√©tricas:**
> - **Precision** baixa: muitos falsos positivos
> - **Recall** razo√°vel: consegue identificar 60% dos casos positivos
> - **F1 Score** baixo: modelo desbalanceado entre precision e recall

---

## Conclus√µes e Aprendizados

Apesar das tentativas de ajuste (varia√ß√£o de componentes, taxa de aprendizado, backtracking), os resultados foram limitados devido ao forte **desbalanceamento** dos dados.

### Poss√≠veis Melhorias

- T√©cnicas de oversampling (SMOTE)
- Ajuste de threshold de decis√£o
- Feature engineering mais elaborado

---

## To Do

- [ ] Implementar Newton-Raphson
- [ ] Implementar BFGS
- [ ] Testar SMOTE para balanceamento
- [ ] Adicionar valida√ß√£o cruzada
